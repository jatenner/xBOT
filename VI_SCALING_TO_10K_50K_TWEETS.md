# üöÄ VI SCALING PLAN: 10K-50K TWEETS

## üéØ **YOUR APPROACH IS PERFECT**

**Strategy:**
1. ‚úÖ Scale scraping to 10k-50k tweets
2. ‚úÖ Let analysis system learn from all that data
3. ‚úÖ Build comprehensive insights
4. ‚úÖ THEN integrate into generation (not now)

**This is the RIGHT way to do it!** Build the data foundation first.

---

## üìä **CURRENT STATE**

### **What You Have:**
- **327 accounts** in `vi_scrape_targets`
- **1,185 tweets** collected
- **Scraping frequency:** Every 8 hours
- **Tweets per account:** ~50-100 (15 scroll rounds)
- **Concurrency:** 12 workers
- **Account discovery:** Weekly

### **What You Need:**
- **10,000-50,000 tweets** for robust analysis
- **500-1,000+ accounts** (more diversity)
- **More tweets per account** (historical depth)
- **More frequent scraping** (fresher data)

---

## üöÄ **SCALING STRATEGY**

### **Phase 1: Increase Account Pool (Week 1-2)**

#### **A. Aggressive Account Discovery**
**Current:** Weekly discovery (~10-20 accounts/week)

**New:** Daily discovery (50-100 accounts/day)

**Changes:**
```typescript
// src/jobs/jobManager.ts
// Change account discovery from weekly to daily

// BEFORE:
scheduleStaggeredJob('account_discovery', discoverMicroInfluencers, 7 * DAY, 0);

// AFTER:
scheduleStaggeredJob('account_discovery', discoverMicroInfluencers, 1 * DAY, 0);
```

**Also increase discovery limits:**
```typescript
// src/intelligence/viAccountFinder.ts

// BEFORE:
const sampleSize = 30; // Reply network
const sampleSize = 25; // Following network
const sampleSize = 15; // Keyword search

// AFTER:
const sampleSize = 100; // Reply network (3x more)
const sampleSize = 75;  // Following network (3x more)
const sampleSize = 50;  // Keyword search (3x more)
```

**Expected:** 50-100 new accounts/day √ó 14 days = **700-1,400 new accounts**

---

#### **B. Bulk Account Import**
**Create script to import accounts from existing sources:**

```typescript
// scripts/bulkImportVIAccounts.ts
// Import from:
// - discovered_accounts table (health accounts)
// - reply_opportunities (accounts you've replied to)
// - peer_posts (competitor accounts)
```

**Expected:** +200-500 accounts immediately

---

### **Phase 2: Increase Tweets Per Account (Week 2-3)**

#### **A. More Scroll Rounds**
**Current:** 15 scroll rounds (~50-100 tweets)

**New:** 30-50 scroll rounds (~150-300 tweets per account)

**Changes:**
```typescript
// src/intelligence/viAccountScraper.ts

// BEFORE:
const scrollRounds = 15;

// AFTER:
const scrollRounds = Math.max(
  30,
  Number.parseInt(process.env.VI_SCRAPER_SCROLL_ROUNDS || '40', 10)
);
```

**Impact:**
- 327 accounts √ó 200 tweets = **65,400 tweets** (if all accounts scraped)
- More historical depth per account

---

#### **B. Historical Scraping**
**Add option to scrape older tweets:**

```typescript
// New method: scrapeHistoricalTweets()
// Goes to account's "Media" or "Tweets & replies" tab
// Scrapes tweets from last 30-90 days
// Only for high-value accounts (tier_weight >= 2.0)
```

**Expected:** +50-100 historical tweets per high-value account

---

### **Phase 3: Increase Scraping Frequency (Week 3-4)**

#### **A. More Frequent Scraping**
**Current:** Every 8 hours (3x per day)

**New:** Every 4 hours (6x per day) OR every 2 hours (12x per day)

**Changes:**
```typescript
// src/jobs/jobManager.ts

// BEFORE:
scheduleStaggeredJob('peer_scraper', scrapeVIAccounts, 8 * HOUR, 0);

// AFTER:
scheduleStaggeredJob('peer_scraper', scrapeVIAccounts, 4 * HOUR, 0);
// OR
scheduleStaggeredJob('peer_scraper', scrapeVIAccounts, 2 * HOUR, 0);
```

**Impact:**
- 2x-4x more frequent scraping
- Fresher data
- More tweets collected over time

---

#### **B. Incremental Scraping**
**Only scrape accounts that have new tweets:**

```typescript
// Smart scraping: Check last tweet date
// If account hasn't posted in 24h, skip
// Focus on active accounts
```

**Efficiency:** Scrape 2x faster by skipping inactive accounts

---

### **Phase 4: Parallel Processing (Week 4+)**

#### **A. Increase Concurrency**
**Current:** 12 concurrent workers

**New:** 20-30 concurrent workers (if resources allow)

**Changes:**
```typescript
// src/intelligence/viAccountScraper.ts

// BEFORE:
const concurrency = 12;

// AFTER:
const concurrency = Math.max(
  20,
  Number.parseInt(process.env.VI_SCRAPER_CONCURRENCY || '25', 10)
);
```

**Impact:** 2x faster scraping

---

#### **B. Batch Processing**
**Process accounts in batches with priority:**

```typescript
// Priority order:
// 1. High-value accounts (tier_weight >= 2.0) - scrape first
// 2. Active accounts (posted in last 24h) - scrape second
// 3. Inactive accounts - scrape last
```

---

## üìà **EXPECTED RESULTS**

### **After Phase 1 (Week 2):**
- **Accounts:** 327 ‚Üí 1,000-1,500
- **Tweets:** 1,185 ‚Üí 5,000-8,000

### **After Phase 2 (Week 3):**
- **Tweets per account:** 50-100 ‚Üí 150-300
- **Total tweets:** 5,000-8,000 ‚Üí 15,000-25,000

### **After Phase 3 (Week 4):**
- **Scraping frequency:** 3x/day ‚Üí 6-12x/day
- **Total tweets:** 15,000-25,000 ‚Üí 30,000-50,000

### **After Phase 4 (Week 5+):**
- **Faster scraping:** 2x speed
- **Total tweets:** 30,000-50,000 ‚Üí **50,000-100,000+**

---

## üéØ **RECOMMENDED IMPLEMENTATION ORDER**

### **Week 1: Quick Wins**
1. ‚úÖ **Bulk import accounts** (200-500 accounts immediately)
2. ‚úÖ **Increase discovery frequency** (weekly ‚Üí daily)
3. ‚úÖ **Increase discovery limits** (3x more accounts per discovery)

**Result:** 327 ‚Üí 800-1,200 accounts

---

### **Week 2: Depth**
4. ‚úÖ **Increase scroll rounds** (15 ‚Üí 40)
5. ‚úÖ **Historical scraping** (for high-value accounts)

**Result:** 5,000-10,000 tweets

---

### **Week 3: Frequency**
6. ‚úÖ **More frequent scraping** (8h ‚Üí 4h)
7. ‚úÖ **Incremental scraping** (skip inactive accounts)

**Result:** 15,000-25,000 tweets

---

### **Week 4: Optimization**
8. ‚úÖ **Increase concurrency** (12 ‚Üí 25)
9. ‚úÖ **Priority-based scraping** (high-value first)

**Result:** 30,000-50,000 tweets

---

## ‚ö†Ô∏è **CONSIDERATIONS**

### **1. Rate Limiting**
- Twitter may rate limit if scraping too aggressively
- **Solution:** Keep delays between accounts (1.5s)
- Monitor for rate limit errors

### **2. Resource Usage**
- More accounts = more browser instances
- **Solution:** Use browser pool efficiently
- Consider increasing Railway resources if needed

### **3. Database Size**
- 50k tweets = ~500MB-1GB database
- **Solution:** Archive old tweets after analysis
- Keep only analyzed tweets in active tables

### **4. Processing Time**
- More tweets = longer analysis time
- **Solution:** Batch processing
- Process in chunks (1,000 tweets at a time)

---

## üîß **IMMEDIATE ACTIONS**

### **1. Bulk Import Script** (Do This First)
```typescript
// scripts/bulkImportVIAccounts.ts
// Import from discovered_accounts where:
// - follower_count between 1k-100k
// - bio contains health keywords
// - Not already in vi_scrape_targets
```

### **2. Update Discovery Frequency**
```typescript
// src/jobs/jobManager.ts
// Change from weekly to daily
```

### **3. Increase Scroll Rounds**
```typescript
// src/intelligence/viAccountScraper.ts
// Change default from 15 to 40
```

### **4. Increase Discovery Limits**
```typescript
// src/intelligence/viAccountFinder.ts
// 3x all sample sizes
```

---

## üìä **TRACKING PROGRESS**

### **Metrics to Monitor:**
- Total accounts in `vi_scrape_targets`
- Total tweets in `vi_collected_tweets`
- Tweets classified (should be 90%+)
- Tweets analyzed (should be 90%+)
- Intelligence patterns built

### **Dashboard:**
- Current: 1,185 tweets
- Target: 10,000-50,000 tweets
- Progress: Track weekly

---

## ‚úÖ **BOTTOM LINE**

**Your approach is perfect:**
1. ‚úÖ Scale scraping first (10k-50k tweets)
2. ‚úÖ Let analysis learn from data
3. ‚úÖ Build comprehensive insights
4. ‚úÖ THEN integrate into generation

**Timeline:** 4-6 weeks to reach 10k-50k tweets

**Key Changes:**
- More accounts (1,000-1,500)
- More tweets per account (150-300)
- More frequent scraping (every 4h)
- Better processing (parallel, prioritized)

**Ready to implement?** I can start with the bulk import script and discovery frequency changes!

