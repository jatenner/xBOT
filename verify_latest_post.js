require('dotenv').config();
const { createClient } = require('@supabase/supabase-js');

const supabase = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY
);

const TWEET_ID = '1980712165599297863'; // Your latest post

async function verifyPost() {
  console.log('ðŸ” VERIFYING LATEST POST SCRAPING\n');
  console.log('='.repeat(80));
  console.log(`\nðŸ“ Tweet: https://x.com/Signal_Synapse/status/${TWEET_ID}`);
  console.log(`ðŸ• Posted: 3:05 PM, Oct 21, 2025\n`);
  
  // Step 1: Check if tweet exists in content_metadata
  console.log('STEP 1: Checking content_metadata table...\n');
  
  const { data: content, error: contentError } = await supabase
    .from('content_metadata')
    .select('decision_id, content, tweet_id, status, posted_at, quality_score')
    .eq('tweet_id', TWEET_ID)
    .single();
  
  if (contentError) {
    console.log('âŒ Tweet NOT found in content_metadata!');
    console.log('   Error:', contentError.message);
    console.log('\nðŸ’¡ This means the tweet was not saved to database.');
    console.log('   Check if posting system is working.\n');
    return;
  }
  
  console.log('âœ… Tweet found in content_metadata!');
  console.log(`   Decision ID: ${content.decision_id}`);
  console.log(`   Content: "${content.content.substring(0, 60)}..."`);
  console.log(`   Status: ${content.status}`);
  console.log(`   Posted at: ${content.posted_at}`);
  console.log(`   Quality score: ${content.quality_score || 'N/A'}/100\n`);
  
  // Step 2: Check if metrics were scraped
  console.log('STEP 2: Checking outcomes table for scraped metrics...\n');
  
  const { data: metrics, error: metricsError } = await supabase
    .from('outcomes')
    .select('*')
    .eq('decision_id', content.decision_id)
    .order('collected_at', { ascending: false })
    .limit(5);
  
  if (metricsError) {
    console.log('âŒ Error querying outcomes:', metricsError.message);
    return;
  }
  
  if (!metrics || metrics.length === 0) {
    console.log('âš ï¸  NO METRICS FOUND YET');
    console.log('   Scraper has not run yet or failed to scrape.');
    console.log('   Wait another 10 minutes and run this script again.\n');
    return;
  }
  
  console.log(`âœ… Found ${metrics.length} metric snapshots:\n`);
  
  for (const snapshot of metrics) {
    const collectedAt = new Date(snapshot.collected_at);
    const minutesAgo = Math.floor((Date.now() - collectedAt.getTime()) / 60000);
    
    const views = snapshot.impressions || snapshot.views || 0;
    const likes = snapshot.likes || 0;
    const retweets = snapshot.retweets || 0;
    const replies = snapshot.replies || 0;
    const profileClicks = snapshot.profile_clicks || 0;
    
    console.log(`ðŸ“Š Snapshot from ${minutesAgo} minutes ago:`);
    console.log(`   ðŸ‘€ Views: ${views.toLocaleString()}`);
    console.log(`   â¤ï¸  Likes: ${likes}`);
    console.log(`   ðŸ”„ Retweets: ${retweets}`);
    console.log(`   ðŸ’¬ Replies: ${replies}`);
    console.log(`   ðŸ‘¤ Profile visits: ${profileClicks}`);
    
    // Validate metrics are realistic
    const isFake = views > 100000;
    const isRealistic = views >= 0 && views <= 10000;
    
    if (isFake) {
      console.log(`   âŒ FAKE DATA - Still scraping from error pages!`);
    } else if (isRealistic) {
      console.log(`   âœ… REALISTIC DATA - Scraper working correctly!`);
    } else {
      console.log(`   âš ï¸  Unusual metrics - verify manually`);
    }
    console.log('');
  }
  
  // Step 3: Compare with your screenshot
  console.log('='.repeat(80));
  console.log('\nðŸŽ¯ VERIFICATION:\n');
  
  const latestMetrics = metrics[0];
  const latestViews = latestMetrics.impressions || latestMetrics.views || 0;
  
  console.log('YOUR SCREENSHOT SHOWS:');
  console.log('  ðŸ‘€ 4 Views');
  console.log('  â¤ï¸  0 Likes\n');
  
  console.log('DATABASE HAS:');
  console.log(`  ðŸ‘€ ${latestViews.toLocaleString()} Views`);
  console.log(`  â¤ï¸  ${latestMetrics.likes || 0} Likes\n`);
  
  if (latestViews === 4) {
    console.log('âœ… PERFECT MATCH! Scraper is working 100% correctly!\n');
  } else if (latestViews === 5000000) {
    console.log('âŒ SCRAPER STILL BROKEN - Getting fake 5M views from error page\n');
    console.log('ðŸ”§ ISSUE: Session auth not working for analytics page\n');
  } else if (latestViews >= 0 && latestViews <= 100) {
    console.log('âœ… CLOSE MATCH - Scraper working! (Views may have increased since screenshot)\n');
  } else {
    console.log('âš ï¸  MISMATCH - Investigate further\n');
  }
  
  // Step 4: Check scraper logs
  console.log('='.repeat(80));
  console.log('\nðŸ’¡ NEXT STEPS:\n');
  
  if (latestViews === 5000000) {
    console.log('1. Analytics auth is STILL not working');
    console.log('2. Need to investigate twitter_session.json');
    console.log('3. May need to re-capture authenticated session\n');
  } else if (metrics.length === 0) {
    console.log('1. Wait 10 more minutes for scraper to run');
    console.log('2. Check logs: railway logs | grep "METRICS_JOB"');
    console.log('3. Verify scraper job is scheduled\n');
  } else if (latestViews >= 0 && latestViews <= 10000) {
    console.log('âœ… Scraper is WORKING! System is healthy!');
    console.log('âœ… Learning system will now have REAL data!');
    console.log('âœ… Quality improvements can be tracked!\n');
  }
}

verifyPost().catch(console.error);

