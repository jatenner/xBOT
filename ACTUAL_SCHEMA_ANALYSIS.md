# üìä ACTUAL DATABASE SCHEMA ANALYSIS

**Date:** November 2, 2025  
**Source:** Migration files inspection  
**Status:** Understanding phase - NO fixes yet

---

## ‚úÖ CONTENT_METADATA TABLE (The Main Queue)

**Purpose:** Queue of all content waiting to be posted (singles, threads, replies)

**Schema (from 20251018_clean_content_metadata.sql):**

```sql
CREATE TABLE content_metadata (
  -- PRIMARY KEY
  id BIGSERIAL PRIMARY KEY,
  decision_id UUID NOT NULL DEFAULT gen_random_uuid() UNIQUE,
  
  -- CONTENT
  content TEXT NOT NULL,
  decision_type TEXT NOT NULL CHECK (decision_type IN ('single', 'thread', 'reply')),
  thread_parts JSONB,  -- ["tweet1", "tweet2", ...]
  
  -- GENERATION METADATA (for learning!)
  generator_name TEXT,              ‚úÖ Learning needs this
  angle TEXT,                       ‚úÖ Learning needs this  
  tone TEXT,                        ‚ùå MISSING! Learning needs this
  format_strategy TEXT,             ‚ùå MISSING! Learning needs this
  visual_format TEXT,               ‚ùå MISSING! Learning needs this
  topic_cluster TEXT,               ‚úÖ Has this (but not 'raw_topic')
  style TEXT,                       ‚úÖ Has this
  hook_type TEXT,                   ‚úÖ Has this
  hook_pattern TEXT,                ‚úÖ Has this
  cta_type TEXT,                    ‚úÖ Has this
  fact_source TEXT,                 ‚úÖ Has this
  
  -- PREDICTIONS
  quality_score NUMERIC(5,4),       ‚úÖ Has this
  predicted_er NUMERIC(5,4),        ‚úÖ Has this
  predicted_engagement TEXT,        ‚úÖ Has this
  
  -- BANDIT TRACKING
  bandit_arm TEXT,                  ‚úÖ Has this
  timing_arm TEXT,                  ‚úÖ Has this
  experiment_id TEXT,               ‚úÖ Has this
  experiment_arm TEXT,              ‚úÖ Has this
  
  -- STATUS & SCHEDULING
  status TEXT NOT NULL DEFAULT 'queued',
  generation_source TEXT NOT NULL,  -- 'real' or 'synthetic'
  scheduled_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  posted_at TIMESTAMPTZ,
  tweet_id TEXT,  -- Filled after posting
  
  -- REPLY FIELDS
  target_tweet_id TEXT,             ‚úÖ Has this
  target_username TEXT,             ‚úÖ Has this
  
  -- ADVANCED
  embedding VECTOR(1536),           -- For duplicate detection
  content_hash TEXT,
  features JSONB,                   -- AI features
  
  -- CONTENT ANALYSIS
  thread_length INTEGER DEFAULT 1,
  fact_count INTEGER DEFAULT 1,
  novelty REAL,
  readability_score REAL,
  sentiment REAL,
  
  -- PERFORMANCE (filled AFTER scraping)
  actual_likes INTEGER,             ‚úÖ Stores scraped likes
  actual_retweets INTEGER,          ‚úÖ Stores scraped retweets
  actual_replies INTEGER,           ‚úÖ Stores scraped replies
  actual_impressions INTEGER,       ‚úÖ Stores scraped impressions
  actual_engagement_rate NUMERIC(5,4), ‚úÖ Calculated
  viral_score INTEGER,
  
  -- LEARNING METRICS (filled by learning system)
  prediction_accuracy NUMERIC(5,4),
  style_effectiveness INTEGER,
  hook_effectiveness NUMERIC(5,4),
  cta_effectiveness NUMERIC(5,4),
  fact_resonance NUMERIC(5,4),
  
  -- ERROR TRACKING
  skip_reason TEXT,
  error_message TEXT,
  
  -- TIMESTAMPS
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
```

### üîç ANALYSIS:

**‚úÖ What it HAS:**
- `decision_id` for tracking
- `decision_type` for singles/threads/replies
- `generator_name` for learning
- `angle` for learning
- `topic_cluster` for learning
- `style`, `hook_type`, `cta_type` for learning
- `target_tweet_id`, `target_username` for replies
- `actual_likes`, `actual_retweets`, etc. for storing scraped metrics

**‚ùå What it's MISSING:**
- `tone` - Learning system needs this!
- `format_strategy` - Learning system needs this!
- `visual_format` - Learning system needs this!
- `raw_topic` - Code uses this but schema has `topic_cluster`

**üí° KEY INSIGHT:**
`content_metadata` is trying to be EVERYTHING:
- Content queue ‚úÖ
- Posted content record ‚úÖ (has `tweet_id`, `posted_at`)
- Metrics storage ‚úÖ (has `actual_likes`, `actual_retweets`)
- Learning insights ‚úÖ (has `prediction_accuracy`, `style_effectiveness`)

**This is why it has 90+ columns!**

---

## ‚ùì CONTENT_GENERATION_METADATA_COMPREHENSIVE

**Status:** Need to check if this table even exists or if it's obsolete

**Questions:**
1. Does this table exist in production?
2. If yes, what columns does it have?
3. Is it identical to `content_metadata`?
4. Which one does code actually use?

**Action needed:** Check if this is just legacy/abandoned table

---

## ‚úÖ POSTED_DECISIONS TABLE

**Purpose:** Record of posted tweets

**Schema (from 20251001_comprehensive_autonomous_system.sql):**

```sql
CREATE TABLE posted_decisions (
  id BIGSERIAL PRIMARY KEY,
  decision_id UUID NOT NULL,        ‚úÖ Links to content_metadata
  tweet_id TEXT NOT NULL,           ‚úÖ Twitter ID
  content TEXT,                     ‚úÖ Denormalized
  decision_type TEXT NOT NULL,      ‚úÖ 'single', 'thread', 'reply'
  target_tweet_id TEXT,             ‚úÖ For replies
  
  -- DENORMALIZED (copied from content_metadata)
  bandit_arm TEXT,
  timing_arm TEXT,
  generation_source TEXT NOT NULL,
  topic_cluster TEXT,
  quality_score NUMERIC(5,4),
  predicted_er NUMERIC(5,4),
  
  posted_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  
  FOREIGN KEY (decision_id) REFERENCES content_metadata(decision_id)
);
```

### üîç ANALYSIS:

**What it HAS:**
- `decision_id` to link back to content_metadata ‚úÖ
- `tweet_id` for scrapers to find ‚úÖ
- `decision_type` to know if single/thread/reply ‚úÖ
- `target_tweet_id` for replies ‚úÖ

**What it's MISSING:**
- `generator_name` - Learning needs this!
- `angle` - Learning needs this!
- `tone` - Learning needs this!
- `style` - Learning needs this!

**üí° KEY INSIGHT:**
`posted_decisions` doesn't have enough metadata for learning!
Learning system would need to JOIN back to `content_metadata` anyway.

**So why do we need this table if `content_metadata` already tracks posted tweets?**

---

## ‚ùì TWEETS & POSTS TABLES

**Need to check:**
1. Do these tables exist?
2. What columns do they have?
3. Are they duplicates of `posted_decisions`?
4. Which one(s) does code actually use?

---

## ‚úÖ OUTCOMES TABLE

**Purpose:** Engagement metrics from Twitter scraping

**Schema (from 20251001_comprehensive_autonomous_system.sql):**

```sql
CREATE TABLE outcomes (
  id BIGSERIAL PRIMARY KEY,
  decision_id UUID NOT NULL,        ‚úÖ Links to content_metadata
  tweet_id TEXT NOT NULL,           ‚úÖ Twitter ID
  
  -- ENGAGEMENT METRICS
  impressions BIGINT DEFAULT 0,     ‚úÖ
  likes BIGINT DEFAULT 0,           ‚úÖ
  retweets BIGINT DEFAULT 0,        ‚úÖ
  replies BIGINT DEFAULT 0,         ‚úÖ
  bookmarks BIGINT DEFAULT 0,       ‚úÖ
  quotes BIGINT DEFAULT 0,          ‚úÖ
  
  -- CALCULATED
  er_calculated NUMERIC(5,4),       ‚úÖ Engagement rate
  
  -- METADATA
  simulated BOOLEAN NOT NULL DEFAULT false,  -- Real vs synthetic
  collected_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  
  FOREIGN KEY (decision_id) REFERENCES content_metadata(decision_id)
);
```

### üîç ANALYSIS:

**What it HAS:**
- `decision_id` to link to content ‚úÖ
- `tweet_id` for identification ‚úÖ
- All engagement metrics ‚úÖ
- `collected_at` for time-series ‚úÖ

**What it's MISSING:**
- `collected_pass` (T+1h, T+24h, T+7d marker) ‚ùå
- `views` (separate from impressions) ‚ùå
- `engagement_rate` (has `er_calculated` instead) ‚úÖ
- NO generation metadata (generator, topic, angle, etc.) ‚ùå

**üí° KEY INSIGHT:**
`outcomes` is PURE metrics - no generation context!
Learning system MUST JOIN to `content_metadata` to get:
- `generator_name`
- `topic_cluster`
- `angle`
- `style`
- etc.

---

## üîç THE DATA FLOW REALITY

### **Current Flow:**

```
1. GENERATION (planJob.ts)
   ‚Üì INSERT
   content_metadata
   ‚îú‚îÄ generator_name
   ‚îú‚îÄ topic_cluster
   ‚îú‚îÄ angle
   ‚îú‚îÄ style
   ‚îú‚îÄ hook_type
   ‚îú‚îÄ status = 'queued'
   ‚îî‚îÄ scheduled_at

2. POSTING (postingQueue.ts)
   ‚Üì SELECT WHERE status='queued' AND scheduled_at <= NOW()
   content_metadata
   ‚Üì Post to Twitter
   ‚Üì Get tweet_id back
   ‚Üì UPDATE content_metadata SET tweet_id, posted_at, status='posted'
   ‚Üì INSERT
   posted_decisions
   ‚îú‚îÄ decision_id (link back)
   ‚îú‚îÄ tweet_id
   ‚îú‚îÄ content
   ‚îî‚îÄ posted_at

3. SCRAPING (metricsScraperJob.ts)
   ‚Üì SELECT FROM posted_decisions WHERE posted_at recent
   ‚Üì Scrape Twitter for each tweet_id
   ‚Üì INSERT/UPDATE
   outcomes
   ‚îú‚îÄ decision_id
   ‚îú‚îÄ tweet_id
   ‚îú‚îÄ likes, retweets, etc.
   ‚îî‚îÄ collected_at
   
   ‚Üì ALSO UPDATE
   content_metadata
   ‚îú‚îÄ actual_likes
   ‚îú‚îÄ actual_retweets
   ‚îî‚îÄ actual_engagement_rate

4. LEARNING (learningSystem.ts)
   ‚Üì SELECT
   content_metadata (has EVERYTHING)
   ‚îú‚îÄ generator_name
   ‚îú‚îÄ topic_cluster
   ‚îú‚îÄ angle
   ‚îú‚îÄ actual_likes
   ‚îú‚îÄ actual_retweets
   ‚îî‚îÄ actual_engagement_rate
   
   ‚Üì OR JOIN
   content_metadata + outcomes
   
   ‚Üì INSERT
   learning_posts / learning_insights
```

---

## üí° KEY FINDINGS

### **Finding #1: content_metadata is a MEGA TABLE**

It's trying to be:
1. Content queue ‚úÖ
2. Posted content record ‚úÖ
3. Metrics storage ‚úÖ
4. Learning insights ‚úÖ

**This is actually SMART if:**
- All columns exist and are used correctly
- Learning system can query it without complex JOINs
- Scrapers write back to it

**But it has issues:**
- Missing `tone`, `format_strategy`, `visual_format`
- Has `topic_cluster` but code uses `raw_topic`
- 90+ columns make it hard to understand

---

### **Finding #2: posted_decisions seems redundant**

**Why does it exist if `content_metadata` already has:**
- `tweet_id` (filled after posting)
- `posted_at` (filled after posting)
- `status = 'posted'` (marked after posting)

**Possible reasons:**
1. Historical: Was created before `content_metadata` grew
2. Separation: Keep "queue" separate from "posted record"
3. Performance: Faster queries on smaller table
4. Denormalization: Avoid querying huge `content_metadata`

**But it lacks metadata learning needs!**

---

### **Finding #3: outcomes is clean but isolated**

`outcomes` only has metrics, no context.

**Learning system queries would need:**
```sql
SELECT 
  cm.generator_name,
  cm.topic_cluster,
  cm.angle,
  cm.style,
  o.likes,
  o.retweets,
  o.er_calculated
FROM content_metadata cm
JOIN outcomes o ON cm.decision_id = o.decision_id
WHERE cm.status = 'posted'
ORDER BY o.er_calculated DESC
```

**OR just query `content_metadata` alone:**
```sql
SELECT 
  generator_name,
  topic_cluster,
  angle,
  style,
  actual_likes,
  actual_retweets,
  actual_engagement_rate
FROM content_metadata
WHERE status = 'posted'
ORDER BY actual_engagement_rate DESC
```

**Second approach is simpler! (if scrapers write to `content_metadata`)**

---

## ‚ùì CRITICAL QUESTIONS TO ANSWER

### **Question 1: What tables actually exist in production?**

Need to verify:
- ‚úÖ `content_metadata` - EXISTS
- ‚ùì `content_generation_metadata_comprehensive` - EXISTS?
- ‚úÖ `posted_decisions` - EXISTS
- ‚ùì `tweets` - EXISTS?
- ‚ùì `posts` - EXISTS?
- ‚úÖ `outcomes` - EXISTS
- ‚ùì `real_tweet_metrics` - EXISTS?
- ‚ùì `tweet_analytics` - EXISTS?
- ‚ùì `tweet_metrics` - EXISTS?

### **Question 2: What does scraper actually write to?**

Check `src/jobs/metricsScraperJob.ts`:
- Does it write to `outcomes`?
- Does it ALSO write to `content_metadata.actual_*` fields?
- Does it write to multiple metrics tables?

### **Question 3: What does learning system actually query?**

Check `src/learning/learningSystem.ts`:
- Does it query `content_metadata` alone?
- Does it JOIN `content_metadata + outcomes`?
- Does it have all columns it needs?

### **Question 4: Column name mismatches?**

Code uses:
- `raw_topic` but schema has `topic_cluster`
- `tone` but schema is MISSING it
- `format_strategy` but schema is MISSING it
- `visual_format` but schema is MISSING it

Are these causing bugs?

---

## üöÄ NEXT STEPS (Still Learning!)

**Step 1:** Check actual files to see what tables they reference

```bash
# What does scraper write to?
grep -n "\.from\|\.insert\|\.update" src/jobs/metricsScraperJob.ts

# What does learning query?
grep -n "\.from\|\.select" src/learning/learningSystem.ts

# What does posting queue use?
grep -n "\.from" src/jobs/postingQueue.ts
```

**Step 2:** Document actual data flows from code

**Step 3:** Identify column name mismatches

**Step 4:** Identify missing columns learning needs

**Step 5:** Map complete picture before proposing fixes

---

**Status:** Analysis in progress  
**Next:** Inspect actual code files  
**No fixes yet!** Still understanding! üß†

