# üîç WHAT'S CAUSING MEMORY TO BE SO HIGH?

## üìä MEMORY BREAKDOWN (451MB Total)

### **1. Browser Operations: ~150-200MB** üî¥ HIGHEST

**What:**
- Each browser context: ~50-100MB
- Current: 3 contexts = ~150-300MB
- Browser instance: ~50MB base
- Pages: ~10-20MB each

**Operations:**
- Posting tweets (opens browser, loads Twitter)
- Scraping metrics (opens browser, loads analytics)
- Reply discovery (opens browser, searches Twitter)
- Account discovery (opens browser, scrapes profiles)

**Why High:**
- Browser contexts don't release memory when "closed"
- Zygote processes hold memory
- Multiple contexts open simultaneously
- Browser heap separate from Node.js heap

**Evidence:**
```
[BROWSER_POOL] Emergency cleanup: 0 contexts closed
Memory: 451MB ‚Üí 451MB (freed 0MB)
```

---

### **2. Database Queries Loading All Data: ~50-80MB** üü° HIGH

**What:**
- Loading ALL posts/tweets at once (no pagination)
- Large arrays kept in memory
- Multiple queries running simultaneously

**Operations:**

#### **Reply Job:**
```typescript
// Loads 100 opportunities at once
const { data: allOpportunities } = await supabaseClient
  .from('reply_opportunities')
  .select('*')
  .limit(100);  // ‚ùå Loads 100 records into memory

// Then filters/sorts in memory
const sortedOpportunities = [...allOpportunities].sort(...);
const highVirality = sortedOpportunities.filter(...).slice(0, 5);
const freshHot = sortedOpportunities.filter(...).slice(0, 10);
```

**Memory:** ~10-20MB per query √ó 5-10 queries = **50-80MB**

#### **Metrics Scraper:**
```typescript
// Loads 15 + 5 + 3 = 23 posts at once
const allPostsRaw = [
  ...(missingMetricsPosts || []),  // 15 posts
  ...(recentPosts || []),          // 5 posts
  ...(historicalPosts || [])       // 3 posts
];
```

**Memory:** ~5-10MB

#### **Plan Job:**
```typescript
// Loads ALL content_metadata records
const { data } = await supabase
  .from('content_metadata')
  .select('content')  // ‚ùå Could be hundreds of records
  .order('created_at', { ascending: false });
```

**Memory:** ~20-50MB (depends on table size)

#### **Structural Diversity Engine:**
```typescript
// Loads ALL posts from database
const allPosts = await this.fetchAllDatabasePosts();
// Fetches from learning_posts + tweets tables
// Could be hundreds/thousands of records
```

**Memory:** ~30-50MB

**Total Database Memory:** **50-80MB**

---

### **3. AI Operations: ~30-50MB** üü° MEDIUM

**What:**
- OpenAI API responses (large JSON)
- Content generation (multiple candidates)
- Learning data (performance metrics)

**Operations:**

#### **Content Generation:**
```typescript
// Generates multiple content candidates
const candidates: Content[] = [];
for (let i = 0; i < 5; i++) {
  const content = await generateContent(...);
  candidates.push(content);  // Keeps all in memory
}
```

**Memory:** ~10-20MB per generation

#### **Learning System:**
```typescript
// Loads training data
const trainingData = await collectTrainingData();
// Could be hundreds of records
```

**Memory:** ~10-20MB

**Total AI Memory:** **30-50MB**

---

### **4. In-Memory Caches: ~20-40MB** üü° MEDIUM

**What:**
- Recent content cache
- Performance metrics cache
- Generator weights cache

**Operations:**

#### **Recent Content Cache:**
```typescript
private recentContent: UsedContent[] = [];
// Stores last 10-20 posts in memory
```

**Memory:** ~5-10MB

#### **Performance Metrics:**
```typescript
private metrics = {
  generatorPerformance: Map<string, number>,
  hourPerformance: Map<string, number>,
  // ... more maps
};
```

**Memory:** ~10-20MB

**Total Cache Memory:** **20-40MB**

---

### **5. Node.js Runtime: ~100-150MB** üü¢ BASE

**What:**
- Node.js heap
- Module loading
- Event loop
- Base runtime

**Memory:** **100-150MB** (unavoidable)

---

## üî• TOP MEMORY CONSUMERS

### **Ranked by Memory Usage:**

| Operation | Memory | Frequency | Total Impact |
|-----------|--------|-----------|-------------|
| **Browser contexts** | 150-200MB | Always | üî¥ HIGHEST |
| **Database queries (all data)** | 50-80MB | Every job | üü° HIGH |
| **AI operations** | 30-50MB | Every generation | üü° MEDIUM |
| **In-memory caches** | 20-40MB | Always | üü° MEDIUM |
| **Node.js runtime** | 100-150MB | Always | üü¢ BASE |

**Total:** **350-520MB** (matches your 451MB!)

---

## üö® SPECIFIC PROBLEMATIC OPERATIONS

### **1. Reply Job - Loads 100 Opportunities**

**File:** `src/jobs/replyJob.ts` line 503-509

**Problem:**
```typescript
const { data: allOpportunities } = await supabaseClient
  .from('reply_opportunities')
  .select('*')  // ‚ùå Loads ALL columns
  .limit(100);  // ‚ùå 100 records √ó ~100KB each = 10MB

// Then creates multiple filtered arrays
const sortedOpportunities = [...allOpportunities];  // Copy 1
const highVirality = sortedOpportunities.filter(...);  // Copy 2
const freshHot = sortedOpportunities.filter(...);  // Copy 3
```

**Memory:** ~10-20MB (original + copies)

**Fix:** Load only needed columns, process in batches

---

### **2. Metrics Scraper - Loads 23 Posts**

**File:** `src/jobs/metricsScraperJob.ts` line 100

**Problem:**
```typescript
const allPostsRaw = [
  ...(missingMetricsPosts || []),  // 15 posts
  ...(recentPosts || []),          // 5 posts
  ...(historicalPosts || [])       // 3 posts
];
// Creates array of 23 posts in memory
```

**Memory:** ~5-10MB

**Fix:** Process in batches, don't combine arrays

---

### **3. Structural Diversity - Loads ALL Posts**

**File:** `src/content/structuralDiversityEngine.ts` line 644

**Problem:**
```typescript
private async fetchAllDatabasePosts(): Promise<any[]> {
  // Fetches from learning_posts table (ALL records)
  const { data: learningPosts } = await admin
    .from('learning_posts')
    .select('content, created_at, likes_count, ...')
    .order('created_at', { ascending: false });
    // ‚ùå NO LIMIT - loads ALL posts!
  
  // Then fetches from tweets table (ALL records)
  const { data: tweets } = await admin
    .from('tweets')
    .select('content, created_at, ...')
    .order('created_at', { ascending: false });
    // ‚ùå NO LIMIT - loads ALL tweets!
  
  return allPosts;  // Could be hundreds/thousands
}
```

**Memory:** ~30-50MB (depends on table size)

**Fix:** Add LIMIT, pagination, or process in batches

---

### **4. Browser Pool - Multiple Contexts**

**File:** `src/browser/UnifiedBrowserPool.ts`

**Problem:**
```typescript
MAX_CONTEXTS = 3  // 3 contexts √ó 50-100MB each = 150-300MB
```

**Memory:** ~150-300MB

**Fix:** Reduce to 2 contexts, better cleanup

---

### **5. Content Generation - Multiple Candidates**

**File:** `src/jobs/planJob.ts`

**Problem:**
```typescript
const candidates: Content[] = [];
for (let i = 0; i < 5; i++) {
  const content = await generateContent(...);
  candidates.push(content);  // Keeps all 5 in memory
}
// Then selects best one, but all 5 still in memory
```

**Memory:** ~10-20MB per generation cycle

**Fix:** Clear candidates after selection

---

## üìä MEMORY ACCUMULATION OVER TIME

### **Startup:**
```
Node.js runtime: 100MB
Modules loaded: +50MB
Browser launched: +50MB
Total: 200MB ‚úÖ
```

### **After 1 Hour:**
```
Base: 200MB
Browser contexts: +150MB (3 contexts)
Database queries: +50MB (accumulated)
AI operations: +30MB (accumulated)
Caches: +20MB (accumulated)
Total: 450MB ‚ùå (88% - CRITICAL)
```

### **Why It Accumulates:**
1. Browser contexts don't release memory
2. Database queries keep arrays in memory
3. Caches grow over time
4. No cleanup between operations

---

## ‚úÖ FIXES BY OPERATION

### **Fix 1: Browser Optimization** (Saves ~100MB)
- Single-process mode (no zygote)
- Lower heap limit (256MB vs 2048MB)
- Better cleanup

**Impact:** 150-200MB ‚Üí 100-150MB

### **Fix 2: Database Pagination** (Saves ~30MB)
- Process in batches of 10-20
- Don't load all data at once
- Clear arrays after processing

**Impact:** 50-80MB ‚Üí 20-50MB

### **Fix 3: Clear Caches** (Saves ~20MB)
- Limit cache size
- Clear after use
- Database-backed caching

**Impact:** 20-40MB ‚Üí 10-20MB

### **Fix 4: Reduce Contexts** (Saves ~50MB)
- MAX_CONTEXTS: 3 ‚Üí 2
- Better reuse
- Faster cleanup

**Impact:** 150-200MB ‚Üí 100-150MB

**Total Savings:** **~200MB** (451MB ‚Üí 250MB)

---

## üéØ SUMMARY

### **What's Clogging Memory:**

1. **Browser contexts** (150-200MB) - Highest
2. **Database queries** (50-80MB) - Loads all data
3. **AI operations** (30-50MB) - Multiple candidates
4. **Caches** (20-40MB) - Grows over time
5. **Node.js runtime** (100-150MB) - Base

### **Why It Stays High:**

- Browser contexts don't release memory
- Database queries load all data at once
- No cleanup between operations
- Memory accumulates over time

### **How to Fix:**

1. Browser optimization (already done) ‚úÖ
2. Database pagination (needs implementation)
3. Clear caches (needs implementation)
4. Reduce contexts (needs implementation)

**Result:** 451MB ‚Üí ~250MB ‚úÖ

