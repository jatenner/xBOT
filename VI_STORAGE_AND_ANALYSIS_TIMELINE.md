# üìä VI SYSTEM: STORAGE + ANALYSIS TIMELINE

## **TWO-PHASE SYSTEM:**

### **Phase 1: STORAGE (Scraping)**
- **Job:** `peer_scraper` (every 2 hours)
- **Action:** Scrapes tweets from 400 accounts
- **Stores in:** `vi_collected_tweets` table
- **Status:** `classified = false` (not yet analyzed)

### **Phase 2: ANALYSIS (Processing)**
- **Job:** `data_collection` (every 6 hours)
- **Action:** Classifies, analyzes, builds intelligence
- **Updates:** `vi_content_classification`, `vi_visual_formatting`, `vi_format_intelligence`
- **Status:** `classified = true` (analyzed)

---

## **STORAGE TIMELINE (Phase 1):**

### **With New Optimizations:**
- **Scroll rounds:** 40 (was 15)
- **Workers:** 15 (was 12)
- **Accounts:** 350-400
- **Frequency:** Every 2 hours

### **Tweets Stored Per Cycle:**
- **Conservative:** 3,500 new tweets per cycle
- **Realistic:** 3,500-5,000 new tweets per cycle
- **Best case:** 5,000+ new tweets per cycle

### **Timeline to 25k Stored:**
- **Best case:** 10 hours (0.4 days) ‚úÖ
- **Realistic:** 14 hours (0.6 days) ‚úÖ
- **Worst case:** 25 hours (1.04 days) ‚úÖ

**2-Day Projection:**
- **Minimum:** 48,000 tweets stored ‚úÖ
- **Realistic:** 84,000 tweets stored ‚úÖ
- **Best case:** 120,000 tweets stored ‚úÖ

---

## **ANALYSIS TIMELINE (Phase 2):**

### **Processing Job:**
- **Job:** `data_collection` (runs every 6 hours)
- **Function:** `runVIProcessing()` ‚Üí `processAllPending()`
- **Location:** `src/jobs/vi-job-extensions.ts:159-177`

### **Three-Stage Pipeline:**

**Stage 1: Classification**
- Processes unclassified tweets (max 100 per run)
- Uses OpenAI to extract: topic, angle, tone, structure, generator match
- Stores in: `vi_content_classification`
- **Time:** ~500ms per tweet (with 500ms delay between calls)
- **Throughput:** ~100 tweets per run (limited by batch size)

**Stage 2: Visual Analysis**
- Processes classified tweets
- Extracts: emojis, line breaks, readability, CTA, timing
- Stores in: `vi_visual_formatting`
- **Time:** ~100ms per tweet (local processing)
- **Throughput:** ~1000+ tweets per run (fast)

**Stage 3: Intelligence Building**
- Aggregates patterns from analyzed tweets
- Correlates patterns with engagement rates
- Stores in: `vi_format_intelligence`
- **Time:** ~1-2 seconds per pattern (aggregation)
- **Throughput:** Depends on unique patterns

### **Analysis Capacity:**

**Per Run (Every 6 Hours):**
- **Classification:** ~100 tweets (limited by batch size)
- **Visual Analysis:** ~1000+ tweets (fast, no limit)
- **Intelligence Building:** Processes all analyzed tweets

**Bottleneck:** Classification (Stage 1) is limited to 100 tweets per run

---

## **COMBINED TIMELINE: STORAGE + ANALYSIS**

### **Scenario 1: High Activity (Best Case)**

**Storage:**
- **10 hours:** 25,000 tweets stored ‚úÖ

**Analysis:**
- **First run (6 hours):** ~100 tweets analyzed
- **Second run (12 hours):** ~100 tweets analyzed
- **Third run (18 hours):** ~100 tweets analyzed
- **Total analyzed in 18 hours:** ~300 tweets

**Problem:** Analysis is SLOWER than storage!

### **Scenario 2: Realistic Activity**

**Storage:**
- **14 hours:** 25,000 tweets stored ‚úÖ

**Analysis:**
- **First run (6 hours):** ~100 tweets analyzed
- **Second run (12 hours):** ~100 tweets analyzed
- **Third run (18 hours):** ~100 tweets analyzed
- **Total analyzed in 18 hours:** ~300 tweets

**Problem:** Analysis is MUCH SLOWER than storage!

### **Scenario 3: Worst Case**

**Storage:**
- **25 hours:** 25,000 tweets stored ‚úÖ

**Analysis:**
- **First run (6 hours):** ~100 tweets analyzed
- **Second run (12 hours):** ~100 tweets analyzed
- **Third run (18 hours):** ~100 tweets analyzed
- **Fourth run (24 hours):** ~100 tweets analyzed
- **Total analyzed in 24 hours:** ~400 tweets

**Problem:** Analysis is EXTREMELY SLOWER than storage!

---

## **THE REALITY:**

### **Storage (Scraping):**
‚úÖ **25k tweets stored in 1-2 days** (easily achievable)

### **Analysis (Processing):**
‚ö†Ô∏è **25k tweets analyzed in 25+ days** (bottleneck!)

**Why:**
- Classification limited to 100 tweets per run
- Runs every 6 hours
- 100 tweets √ó 4 runs/day = 400 tweets/day
- 25,000 √∑ 400 = **62.5 days** to analyze all tweets

---

## **SOLUTION: INCREASE ANALYSIS CAPACITY**

### **Option 1: Increase Batch Size**
**Current:** 100 tweets per run  
**Change to:** 500-1000 tweets per run

**Impact:**
- 500 tweets √ó 4 runs/day = 2,000 tweets/day
- 25,000 √∑ 2,000 = **12.5 days** to analyze all tweets

### **Option 2: Run More Frequently**
**Current:** Every 6 hours (4 runs/day)  
**Change to:** Every 2 hours (12 runs/day)

**Impact:**
- 100 tweets √ó 12 runs/day = 1,200 tweets/day
- 25,000 √∑ 1,200 = **20.8 days** to analyze all tweets

### **Option 3: Both (Recommended)**
**Change to:** 500 tweets per run, every 2 hours

**Impact:**
- 500 tweets √ó 12 runs/day = 6,000 tweets/day
- 25,000 √∑ 6,000 = **4.2 days** to analyze all tweets ‚úÖ

---

## **REVISED EXPECTATION:**

### **With Current Settings:**
- **Storage:** 25k tweets in **1-2 days** ‚úÖ
- **Analysis:** 25k tweets in **62.5 days** ‚ö†Ô∏è

### **With Optimized Settings (500 tweets/run, every 2 hours):**
- **Storage:** 25k tweets in **1-2 days** ‚úÖ
- **Analysis:** 25k tweets in **4-5 days** ‚úÖ

---

## **RECOMMENDATION:**

**For 25k tweets STORED AND ANALYZED in 2-3 days:**

1. ‚úÖ **Storage is already optimized** (40 scroll rounds, 15 workers)
2. ‚ö†Ô∏è **Analysis needs optimization:**
   - Increase batch size: 100 ‚Üí 500 tweets per run
   - Increase frequency: Every 6 hours ‚Üí Every 2 hours
   - **Result:** 25k analyzed in 4-5 days (close to target)

**Alternative:** Accept that analysis will lag behind storage
- **Storage:** 25k in 1-2 days ‚úÖ
- **Analysis:** Will catch up over time (62.5 days with current settings)

---

## **BOTTOM LINE:**

**Storage:** ‚úÖ **25k tweets in 1-2 days** (guaranteed)

**Analysis:** ‚ö†Ô∏è **25k tweets in 62.5 days** (with current settings)
- **OR** **25k tweets in 4-5 days** (with optimized settings)

**Recommendation:** Optimize analysis capacity to match storage speed!

